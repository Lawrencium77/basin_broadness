{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has been lifted from the [PyTorch examples](https://github.com/pytorch/examples/blob/main/mnist/main.py)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from hessian_eigenthings import compute_hessian_eigenthings\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 1000\n",
    "LR = 1.0\n",
    "GAMMA = 0.7\n",
    "EPOCHS = 5\n",
    "SAVE_MODEL = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basin Broadness\n",
    "Calculated using equation that can be found in [this LessWrong post](https://www.lesswrong.com/posts/QPqztHpToij2nx7ET/hessian-and-basin-volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hessian_spectrum(model):\n",
    "    pass\n",
    "\n",
    "def get_c(init_std):\n",
    "    pass\n",
    "\n",
    "def get_basin_volume(model, _lambda, init_std):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, dry_run=False, log_interval=100):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.351011\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.279978\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.257797\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.316638\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.157406\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.312547\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.164181\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.327729\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.201208\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.241340\n",
      "\n",
      "Test set: Average loss: -6.5851, Accuracy: 9404/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.090885\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.137226\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.084006\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.268485\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.116145\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.213458\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.100226\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.276355\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.153515\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.175841\n",
      "\n",
      "Test set: Average loss: -5.6060, Accuracy: 9538/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.066961\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.092961\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.068127\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.274245\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.098734\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.150046\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.080226\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.234520\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.161152\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.159940\n",
      "\n",
      "Test set: Average loss: -4.7709, Accuracy: 9566/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.063985\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.067761\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.056863\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.266760\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.096416\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.123678\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.065417\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.236411\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.154663\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.155206\n",
      "\n",
      "Test set: Average loss: -4.1080, Accuracy: 9600/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.056864\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.057328\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.048494\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.268608\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.096581\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.107669\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.050968\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.236217\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.143704\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.151378\n",
      "\n",
      "Test set: Average loss: -3.5941, Accuracy: 9607/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "torch.manual_seed(SEED) # for reproducibility\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': BATCH_SIZE}\n",
    "test_kwargs = {'batch_size': TEST_BATCH_SIZE}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                    transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "model = Net(hidden_dim=24).to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=LR)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=GAMMA)\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================================>...]  Step: 504ms | Tot: 9s669ms | power iter error: 0.00 20/20 \n",
      " [=============================================================>...]  Step: 501ms | Tot: 9s653ms | power iter error: 0.00 20/20 \n",
      " [=============================================================>...]  Step: 506ms | Tot: 9s651ms | power iter error: 0.00 20/20 \n",
      " [===>.............................................................]  Step: 511ms | Tot: 511ms | power iter error: 1.00 2/20 0 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_eigenthings \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      2\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 3\u001b[0m eigenvals, eigenvecs \u001b[39m=\u001b[39m compute_hessian_eigenthings(model\u001b[39m.\u001b[39;49mcpu(), test_loader, loss, num_eigenthings, use_gpu\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/__init__.py:68\u001b[0m, in \u001b[0;36mcompute_hessian_eigenthings\u001b[0;34m(model, dataloader, loss, num_eigenthings, full_dataset, mode, use_gpu, fp16, max_possible_gpu_samples, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m eigenvals, eigenvecs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpower_iter\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 68\u001b[0m     eigenvals, eigenvecs \u001b[39m=\u001b[39m deflated_power_iteration(\n\u001b[1;32m     69\u001b[0m         hvp_operator, num_eigenthings, use_gpu\u001b[39m=\u001b[39;49muse_gpu, fp16\u001b[39m=\u001b[39;49mfp16, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlanczos\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     72\u001b[0m     eigenvals, eigenvecs \u001b[39m=\u001b[39m lanczos(\n\u001b[1;32m     73\u001b[0m         hvp_operator, num_eigenthings, use_gpu\u001b[39m=\u001b[39muse_gpu, fp16\u001b[39m=\u001b[39mfp16, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     74\u001b[0m     )\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/power_iter.py:43\u001b[0m, in \u001b[0;36mdeflated_power_iteration\u001b[0;34m(operator, num_eigenthings, power_iter_steps, power_iter_err_threshold, momentum, use_gpu, fp16, to_numpy)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_eigenthings):\n\u001b[1;32m     42\u001b[0m     utils\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mcomputing eigenvalue/vector \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, num_eigenthings))\n\u001b[0;32m---> 43\u001b[0m     eigenval, eigenvec \u001b[39m=\u001b[39m power_iteration(\n\u001b[1;32m     44\u001b[0m         current_op,\n\u001b[1;32m     45\u001b[0m         power_iter_steps,\n\u001b[1;32m     46\u001b[0m         power_iter_err_threshold,\n\u001b[1;32m     47\u001b[0m         momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m     48\u001b[0m         use_gpu\u001b[39m=\u001b[39;49muse_gpu,\n\u001b[1;32m     49\u001b[0m         fp16\u001b[39m=\u001b[39;49mfp16,\n\u001b[1;32m     50\u001b[0m         init_vec\u001b[39m=\u001b[39;49mprev_vec,\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     utils\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39meigenvalue \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, eigenval))\n\u001b[1;32m     54\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m_new_op_fn\u001b[39m(x, op\u001b[39m=\u001b[39mcurrent_op, val\u001b[39m=\u001b[39meigenval, vec\u001b[39m=\u001b[39meigenvec):\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/power_iter.py:108\u001b[0m, in \u001b[0;36mpower_iteration\u001b[0;34m(operator, steps, error_threshold, momentum, use_gpu, fp16, init_vec)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[1;32m    107\u001b[0m     prev_vec \u001b[39m=\u001b[39m vec \u001b[39m/\u001b[39m (torch\u001b[39m.\u001b[39mnorm(vec) \u001b[39m+\u001b[39m \u001b[39m1e-6\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m     new_vec \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mmaybe_fp16(operator\u001b[39m.\u001b[39;49mapply(vec), fp16) \u001b[39m-\u001b[39m momentum \u001b[39m*\u001b[39m prev_vec\n\u001b[1;32m    109\u001b[0m     \u001b[39m# need to handle case where we end up in the nullspace of the operator.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[39m# in this case, we are done.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mnorm(new_vec)\u001b[39m.\u001b[39mitem() \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/operator.py:29\u001b[0m, in \u001b[0;36mLambdaOperator.apply\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_fn(x)\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/power_iter.py:55\u001b[0m, in \u001b[0;36mdeflated_power_iteration.<locals>._new_op_fn\u001b[0;34m(x, op, val, vec)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_new_op_fn\u001b[39m(x, op\u001b[39m=\u001b[39mcurrent_op, val\u001b[39m=\u001b[39meigenval, vec\u001b[39m=\u001b[39meigenvec):\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mmaybe_fp16(op\u001b[39m.\u001b[39;49mapply(x), fp16) \u001b[39m-\u001b[39m _deflate(x, val, vec)\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/operator.py:29\u001b[0m, in \u001b[0;36mLambdaOperator.apply\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_fn(x)\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/power_iter.py:55\u001b[0m, in \u001b[0;36mdeflated_power_iteration.<locals>._new_op_fn\u001b[0;34m(x, op, val, vec)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_new_op_fn\u001b[39m(x, op\u001b[39m=\u001b[39mcurrent_op, val\u001b[39m=\u001b[39meigenval, vec\u001b[39m=\u001b[39meigenvec):\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mmaybe_fp16(op\u001b[39m.\u001b[39;49mapply(x), fp16) \u001b[39m-\u001b[39m _deflate(x, val, vec)\n",
      "    \u001b[0;31m[... skipping similar frames: deflated_power_iteration.<locals>._new_op_fn at line 55 (5 times), LambdaOperator.apply at line 29 (5 times)]\u001b[0m\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/operator.py:29\u001b[0m, in \u001b[0;36mLambdaOperator.apply\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_fn(x)\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/power_iter.py:55\u001b[0m, in \u001b[0;36mdeflated_power_iteration.<locals>._new_op_fn\u001b[0;34m(x, op, val, vec)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_new_op_fn\u001b[39m(x, op\u001b[39m=\u001b[39mcurrent_op, val\u001b[39m=\u001b[39meigenval, vec\u001b[39m=\u001b[39meigenvec):\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mmaybe_fp16(op\u001b[39m.\u001b[39;49mapply(x), fp16) \u001b[39m-\u001b[39m _deflate(x, val, vec)\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/hvp_operator.py:63\u001b[0m, in \u001b[0;36mHVPOperator.apply\u001b[0;34m(self, vec)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39mReturns H*vec where H is the hessian of the loss w.r.t.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mthe vectorized model parameters\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_dataset:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_full(vec)\n\u001b[1;32m     64\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_batch(vec)\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/hvp_operator.py:94\u001b[0m, in \u001b[0;36mHVPOperator._apply_full\u001b[0;34m(self, vec)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m hessian_vec_prod \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m         hessian_vec_prod \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_batch(vec)\n\u001b[1;32m     95\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m         hessian_vec_prod \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_batch(vec)\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/hvp_operator.py:73\u001b[0m, in \u001b[0;36mHVPOperator._apply_batch\u001b[0;34m(self, vec)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39m# compute original gradient, tracking computation graph\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad()\n\u001b[0;32m---> 73\u001b[0m grad_vec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_grad()\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad()\n\u001b[1;32m     75\u001b[0m \u001b[39m# take the second gradient\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m# this is the derivative of <grad_vec, v> where <,> is an inner product.\u001b[39;00m\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/hessian_eigenthings/hvp_operator.py:113\u001b[0m, in \u001b[0;36mHVPOperator._prepare_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39mCompute gradient w.r.t loss over all parameters and vectorize\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     all_inputs, all_targets \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader_iter)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader)\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/git/basin_broadness/.venv/lib/python3.11/site-packages/torchvision/datasets/mnist.py:138\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, Any]:\n\u001b[1;32m    131\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m        index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     img, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index], \u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets[index])\n\u001b[1;32m    140\u001b[0m     \u001b[39m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# to return a PIL Image\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_eigenthings = 10\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "eigenvals, eigenvecs = compute_hessian_eigenthings(model.cpu(), test_loader, loss, num_eigenthings, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
